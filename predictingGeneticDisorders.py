# -*- coding: utf-8 -*-
"""predictingGeneticDisorders.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hQ_hMK4YULmpz9acujHbgr8IYRd2Zyoe

Installing PySpark enviroment
"""

!pip install -q pyspark

from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql.functions import col

conf = SparkConf().setAppName('FinalProject3200')
sc = SparkContext.getOrCreate(conf = conf)
spark = SparkSession.builder.getOrCreate()

sqlContext = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

"""Importing libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import polyfit

"""Uploading the data"""

train_data = pd.read_csv("train.csv")

"""**Cleaning Data**

Dropping columns that will not be revelant in predicting disorders and protected information such as the Father's Name, Patient First Name, and Institue Name. We will also drop "Genetic Disorder" to focus on the subclass of the disorders.
"""

drop_columns = ['Patient Id', 
                'Patient First Name', 
                'Family Name', 
                "Father's name", 
                "Institute Name", 
                "Location of Institute", 
                "Status", 
                "Parental consent",
                "Genetic Disorder"]

train_data.drop(columns = drop_columns, inplace=True)

"""Checking the data types on the columns.
Object representing strings.
float64 representing numeric values.
"""

train_data.dtypes

"""Counting NA in Disorder Subclass"""

subclass_na_count = train_data["Disorder Subclass"].isna().sum()
print(subclass_na_count)

"""We will drop the NA values for Disorder Subclass since we need the genetic disorder to build a proper model."""

train_data = train_data.dropna(subset = ['Disorder Subclass'])

"""Checking NAs has been dropped"""

subclass_na = train_data["Disorder Subclass"].isna().sum()
print(subclass_na)

"""Replace 'NA' values with median in numeric variables since it will not impact the summary statistics as much as the mean would, and allow for the rows to remain valuable.

Also, converting some of the float types to int types.
"""

# Replacing NA in No. of Previous of Abortions with the median of the variable
train_data["No. of previous abortion"].fillna(train_data["No. of previous abortion"].median(), inplace=True)
train_data["No. of previous abortion"] = train_data["No. of previous abortion"].astype("int64")

# Replacing NA in White Blood cell count (thousand per microliter) with the median of the variable
train_data["White Blood cell count (thousand per microliter)"].fillna(train_data["White Blood cell count (thousand per microliter)"].median(), inplace=True)

# Replacing NA in Patient's age with the median of the variable
train_data["Patient Age"].fillna(train_data["Patient Age"].median(), inplace=True)
train_data["Patient Age"] = train_data["Patient Age"].astype("int64")

# Replacing NA in Blood cell count with the median of the variable
train_data["Blood cell count (mcL)"].fillna(train_data["Blood cell count (mcL)"].median(), inplace=True)

# Replacing NA in Mother's age with the median of the variable
train_data["Mother's age"].fillna(train_data["Mother's age"].median(), inplace=True)
train_data["Mother's age"] = train_data["Mother's age"].astype("int64")

# Replacing NA in Father's age with the median of the variable
train_data["Father's age"].fillna(train_data["Father's age"].median(), inplace=True)
train_data["Father's age"] = train_data["Father's age"].astype("int64")

"""Replacing 'NA' values in the categorical variables with an "Unknown" value."""

train_data[["Blood test result", "Birth defects", "Gender", "Heart Rate (rates/min", "Respiratory Rate (breaths/min)", "Follow-up", "Place of birth"]] = train_data[["Blood test result", "Birth defects", "Gender", "Heart Rate (rates/min", "Respiratory Rate (breaths/min)", "Follow-up", "Place of birth"]].fillna('Unknown')

"""Adjusting the categorical variables from object type to categorical type. """

train_data[["Blood test result", "Birth defects", "Gender", "Heart Rate (rates/min", "Respiratory Rate (breaths/min)", "Follow-up", "Place of birth", "Disorder Subclass"]].astype("category")

"""Formating all "-", "Not applicable", "Not available", and "No record" into "No" to create a uniform response. """

train_data = train_data.replace(["-", "Not applicable", "Not available", "None", "No record"], "No")

"""Converting the binary columns from "Yes" and "No" to binary (1, 0)"""

binary_var = ["Maternal gene", "Genes in mother's side",
              "Inherited from father", "Paternal gene",
              "Birth asphyxia", 
              "Folic acid details (peri-conceptional)",
              "H/O serious maternal illness", 
              "H/O radiation exposure (x-ray)",
              "H/O substance abuse", "Assisted conception IVF/ART",
              "History of anomalies in previous pregnancies",
              "Autopsy shows birth defect (if applicable)",
              ]

train_data[binary_var] = train_data[binary_var].replace({"Yes": 1, "No": 0})

"""Replacing 'NA' values in all binary variables with 0 since we are assuming they didn't have the gene, symptom, test, etc.

Transforming binary variables to type "int64".
"""

binary = ["Maternal gene", "Genes in mother's side",
          "Inherited from father", "Paternal gene",
          "Birth asphyxia", 
          "Folic acid details (peri-conceptional)",
          "H/O serious maternal illness", 
          "H/O radiation exposure (x-ray)",
          "H/O substance abuse", "Assisted conception IVF/ART",
          "History of anomalies in previous pregnancies",
          "Autopsy shows birth defect (if applicable)",
          "Test 1", "Test 2", "Test 3", "Test 4", "Test 5",
          "Symptom 1", "Symptom 2", "Symptom 3", "Symptom 4", "Symptom 5"
          ]

# Replacing NAs with 0          
train_data[binary] = train_data[binary].fillna(0)

# Transforming into type int
train_data[binary] = train_data[binary].astype("int64")

"""**Exploring the Data**

Summary of statistics from all the numeric variables in the dataframe.
"""

train_data.describe()

"""Counting the different genetic disorder"""

train_data["Disorder Subclass"].value_counts()

"""We can see that Leigh syndrome has the most instances with 5160. While Cancer has the least with only 97.

Creating new columns to represent if the patient had a certain genetic disorder.
"""

# If the patient has Leigh syndrome
train_data['leigh_syndrome'] = np.where(train_data['Disorder Subclass'] == "Leigh syndrome", 1, 0)

# If the patient has Mitochondrial myopathy
train_data['mitochondrial_myopathy'] = np.where(train_data['Disorder Subclass'] == "Mitochondrial myopathy", 1, 0)

# If the patient has Cystic fibrosis
train_data['cystic_fibrosis'] = np.where(train_data['Disorder Subclass'] == "Cystic fibrosis", 1, 0)

# If the patient has Tay-Sachs
train_data['tay_sachs'] = np.where(train_data['Disorder Subclass'] == "Tay-Sachs", 1, 0)

# If the patient has Diabetes
train_data['diabetes'] = np.where(train_data['Disorder Subclass'] == "Diabetes", 1, 0)

# If the patient has Hemochromatosis
train_data['hemochromatosis'] = np.where(train_data['Disorder Subclass'] == "Hemochromatosis", 1, 0)

# If the patient has Leber's herditary optic neuropathy
train_data['lebers'] = np.where(train_data['Disorder Subclass'] == "Leber's hereditary optic neuropathy", 1, 0)

# If the patient has Alzheimer's
train_data['alzheimers'] = np.where(train_data['Disorder Subclass'] == "Alzheimer's", 1, 0)

# If the patient has Cancer
train_data['cancer'] = np.where(train_data['Disorder Subclass'] == "Cancer", 1, 0)

"""Saving edited dataframe to a CSV file"""

train_data.to_csv('train_data.csv')

"""**Creating a Recommendation System using Collabortative Filtering with Binary Data**

Steps from: https://medium.com/radon-dev/item-item-collaborative-filtering-with-binary-or-unary-data-e8f0b465b2c3

We will use an item-item collaborative filter recommendation system since our focus is being able to predict the diseases by what symptoms and variables are most similar to them, compared to a user-item recommendation system.

Creating a new data frame with just binary variables.
"""

df = train_data[["Maternal gene", "Genes in mother's side",
          "Inherited from father", "Paternal gene",
          "Birth asphyxia", 
          "Folic acid details (peri-conceptional)",
          "H/O serious maternal illness", 
          "H/O radiation exposure (x-ray)",
          "H/O substance abuse", "Assisted conception IVF/ART",
          "History of anomalies in previous pregnancies",
          "Autopsy shows birth defect (if applicable)",
          "Test 1", "Test 2", "Test 3", "Test 4", "Test 5",
          "Symptom 1", "Symptom 2", "Symptom 3", "Symptom 4", "Symptom 5",
          "leigh_syndrome", "mitochondrial_myopathy", "cystic_fibrosis", "tay_sachs",
          "diabetes", "hemochromatosis", "lebers", "alzheimers", "cancer"]].copy()

"""Importing cosine_similarity and sparse libraries"""

from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse

"""**Item-Item Calculations**

Normalizing the user vectors to unit vectors
"""

# magnitude = sqrt(x2 + y2 + z2 + ...)
magnitude = np.sqrt(np.square(df).sum(axis=1))

# unit vector = (x/ magnitude, y/magnitude, z/magnitude, ...)
df = df.divide(magnitude, axis='index')

"""Function to calculate the column-wise cosine similarity for a spare matrix and return a new dataframe matrix with the similarities. 

"""

def calculate_sim(df):
  df_sparse = sparse.csr_matrix(df) # Dealing with the sparse data, CSR is to compress the sparse rows.
  similarities = cosine_similarity(df_sparse.transpose())
  sim = pd.DataFrame(data=similarities, index = df.columns, columns= df.columns)
  return sim

"""Creating a similarity matrix"""

df_matrix = calculate_sim(df)

df_matrix

"""We can see the top 5 variables most similar or related to each disease.

We use 6 since the most similar will always be the variable we are looking at.

*Leigh Syndrome's Top 5 Most Similar Variables*
"""

df_matrix.loc['leigh_syndrome'].nlargest(6)

"""We can see that Test 4, Genes in mother's side, Symptom 1, Symptom 3, and Sympton 2 are most related to Leigh Syndrome. So possibly if a patient had these results, they could be diagnosed with Leigh syndrome. 

Leigh syndrome can only be inherited through the mother, so it explains the importance of having "genes in mother's side" as an indicator (https://medlineplus.gov/genetics/condition/leigh-syndrome/#inheritance).

*Mitochondrial Myopathy's Top 5 Most Similar Variables*
"""

df_matrix.loc['mitochondrial_myopathy'].nlargest(6)

"""Similar to Leigh syndrome, Mitochondrial myopathy is very similar to Test 4 and Genes in mother's side. Which makes sense since Leigh syndrome is a form of Mitochondrial myopathy (https://www.chop.edu/conditions-diseases/mitochondrial-myopathy#:~:text=Mitochondrial%20myopathies%20may%20be%20caused,found%20in%20cells'%20mitochondria).)

Also both Leigh syndrome and Mitochondrial Myopathy are most often passed down through the mother, which supports "Genes in mother's side" being so prevalant in both diseases.

*Cystic Fibrosis' Top 5 Most Similar Variables*
"""

df_matrix.loc['cystic_fibrosis'].nlargest(6)

"""With Cystic Fibrosis, we see that having the symptoms 2, 3, 4, and 5 and testing positive for test 4 could lead help lead to a diagnosis with a patient. Cystic Fibrosis impacts the lungs, pancreas, and other organs, there is very specific symptoms which could explain the high similarity rate (https://www.cff.org/intro-cf/about-cystic-fibrosis).

*Tay-Sachs' Top 5 Most Similar Variables*
"""

df_matrix.loc['tay_sachs'].nlargest(6)

"""We see that having a positive test result with Test 4 could a good indicator if a patient has Tay Sachs.

It is also interesting to see that it seems to relate more to events that happened while the patient was in the womb or being conceived. 

Tay-Sachs must be passed down through both parents, and can have a delayed onset of symptoms which may be why the test result and events of the pregnancy are more prevalent. (https://www.betterhealth.vic.gov.au/health/conditionsandtreatments/tay-sachs-disease)

*Diabetes' Top 5 Most Similar Variables*
"""

df_matrix.loc['diabetes'].nlargest(6)

"""Symptoms 2, 3, 4, and 5 are important indicators when it comes to diagnosing diabetes.

There isn't an exact known cause of childhood diabetes, genetics and family history do play a roll in a child's chances of developing diabetes, which could explain "Genes in mother's side". So it is important for patients and their parents to be aware of the symptoms of diabetes (https://www.mayoclinic.org/diseases-conditions/type-1-diabetes-in-children/symptoms-causes/syc-20355306).

*Hemochromatosis' Top 5 Most Similar Variables*
"""

df_matrix.loc['hemochromatosis'].nlargest(6)

"""Test 4 seems to be the strongest indicator of Hemochromatosis. Usually, hemochromatosis doesn't develop until later in life and some people may never have symptoms of it at all, which could explain the small number of instances in the dataset (https://www.mayoclinic.org/diseases-conditions/hemochromatosis/symptoms-causes/syc-20351443). We also see more similarties to variables regarding previous pregnancy or events that could happen while the patient was in the womb.

*Leber's Top 5 Most Similar Variables*
"""

df_matrix.loc['lebers'].nlargest(6)

"""As we head into the diseases with the smallest amount of instances, we can see that the similarity numbers drop significantly compared to the other diseases with a higher amount of instances. 

In Leber's, symptoms 1, 2, 3, 4, and 5 have the most similarity, which is similar to diabetes. 

Leber's hereditary optic neuropathy is also a mitochondrial disease so it must be inherited through the mother. Usually onset of the disease is not until 15 years old which could explain the small number of instances in the dataset. (https://my.clevelandclinic.org/health/diseases/15620-leber-hereditary-optic-neuropathy-sudden-vision-loss)

*Alzheimer's Top 5 Most Similar Variables*
"""

df_matrix.loc['alzheimers'].nlargest(6)

"""Alzheimer's is most similar to symptoms 2, 3, 4, and 5 like Leber's or diabetes. There is also similarity regarding genes in the mother's side as well. The similarity number have become much smaller, since the number of instances has greatly decreased. 

Alzheimer's using occurs later in life and is usually caused by proteins built up in the brain. This could explain the small amount of instances in the dataset. (https://www.nhs.uk/conditions/alzheimers-disease/causes/#:~:text=Alzheimer's%20disease%20is%20thought%20to,form%20tangles%20within%20brain%20cells.)

*Cancer's Top 5 Most Similar Variables*
"""

df_matrix.loc['cancer'].nlargest(6)

"""Lastly, we look at cancer which has the fewest number of instances in the entire dataset with only 97. So our similarity numbers are the smallest. 

We see that Test 4, history of substance abuse in the mother, history of anomalies in previous pregnancies, assisted conception, and folic acid details could play a part in a diagnosis of cancer. The dataset doesn't specify which type of cancer, but we know in general cancer in childhood is rare, but is the leading cause of death by disease past infancy in the United States (https://www.cancer.gov/types/childhood-cancers/child-adolescent-cancers-fact-sheet#how-common-is-cancer-in-children-and-adolescents).

**Decision Tree Classification**

Implementing a decision tree using Pyspark methods.

Steps from: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html

Loading libraries
"""

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import StringIndexer
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from sklearn.metrics import confusion_matrix

"""Importing edited CSV into Spark."""

data = spark.read.csv("train_data.csv", inferSchema=True, header=True)
# data.show()

"""Counting the Genetic Disorders."""

data.groupBy('Disorder Subclass').count().show()

"""Showing the columns of the dataset."""

data.columns

"""Creating an assembler using Vector Assembler to create a new column to keep track of the independent variables."""

assembler = VectorAssembler(inputCols= ['Patient Age',
 "Genes in mother's side",
 'Inherited from father',
 'Maternal gene',
 'Paternal gene',
 'Blood cell count (mcL)',
 "Mother's age",
 "Father's age",
 'Test 1',
 'Test 2',
 'Test 3',
 'Test 4',
 'Test 5',
 'Birth asphyxia',
 'Autopsy shows birth defect (if applicable)',
 'Folic acid details (peri-conceptional)',
 'H/O serious maternal illness',
 'H/O radiation exposure (x-ray)',
 'H/O substance abuse',
 'Assisted conception IVF/ART',
 'History of anomalies in previous pregnancies',
 'White Blood cell count (thousand per microliter)',
 'Symptom 1',
 'Symptom 2',
 'Symptom 3',
 'Symptom 4',
 'Symptom 5'], outputCol= "features")

assembler

"""Applying the vector assembler to the data."""

output = assembler.transform(data)
output.show()

"""**Leigh Syndrome Decision Tree**

We will focus on creating a decision tree for Leigh Syndrome.
"""

output.select("features", "leigh_syndrome").show()

"""We will create a model using the two columns to represent all the features and the disorder subclasses."""

model_df = output.select("features", "leigh_syndrome")

# model_df.show()

"""Splitting edited data into training and testing data.

70% of the original data in training dataframe and 30% in testing dataframe, we will also set the seed to 1000 to have consistent results. 
"""

training_df, test_df = model_df.randomSplit([0.7, 0.3], 1000)

"""Count of training data."""

training_df.count()

"""Count of test data. """

test_df.count()

"""We will build the Decision Tree Classifer using libraries previously imported libraries and the training dataframe."""

df_classifier = DecisionTreeClassifier(labelCol = "leigh_syndrome").fit(training_df)

"""We will also create predictions using the test dataframe we created."""

df_predictions = df_classifier.transform(test_df)

"""Showing the predictions within the dataframe."""

df_predictions.show()

"""*Evaluating the model*

First, we look at the accuracy of the model using libraries previously imported.
"""

df_accuracy = MulticlassClassificationEvaluator(labelCol="leigh_syndrome",
                                                metricName= "accuracy").evaluate(df_predictions)
df_accuracy

"""The accuracy of the model is the number of correct predictions over the total number of predictions (TP + TN/ TP + TN + FP + FN)

This model about 74% accurate for predicting Leigh syndrome.

Now, the precision of the model
"""

df_precision = MulticlassClassificationEvaluator(labelCol = "leigh_syndrome",
                                                 metricName = "weightedPrecision").evaluate(df_predictions)

df_precision

"""Weighted precision focuses on how many times the patient was correctly diagnosed with leigh syndrome, and ignores the instances where the patient was not diagnosed with leigh syndrome. (TP/ TP + FP)

The weighted precision of the model is about 62%, which is not very high.

We will look at the important variables that go into this prediction.
"""

df_classifier.featureImportances

"""The features that are important in this model are the user ID, patient age, paternal gene, blood cell count, mother's age, test 4, follow up, Folic acid details (peri-conceptional), H/O substance abuse, and Assisted conception IVF/ART.

Lastly, we will look at the AUC for our model.

The area under the ROC curve to measure the performance across all possible classification thresholds.
"""

df_auc = MulticlassClassificationEvaluator(labelCol = "leigh_syndrome").evaluate(df_predictions)
df_auc

"""The AUC of this model is only about 64%, which is also not very high. Possibly the use of different variables or a larger amount of instances may improve this model.

Following the steps used to create the decision tree for Leigh Syndrome, we will apply it to the other genetic disorder subclasses.

**Mitochondrial Myopathy Decision Tree**
"""

model_df1 = output.select("features", "mitochondrial_myopathy")

training_df1, test_df1 = model_df1.randomSplit([0.7, 0.3], 1000)

df_classifier1 = DecisionTreeClassifier(labelCol = "mitochondrial_myopathy").fit(training_df1)

df_predictions1 = df_classifier1.transform(test_df1)

df_accuracy1 = MulticlassClassificationEvaluator(labelCol="mitochondrial_myopathy",
                                                metricName= "accuracy").evaluate(df_predictions1)

df_accuracy1

"""The accuracy of this model is about 78% for being able to predict Mitochondrial Myopathy."""

df_precision1 = MulticlassClassificationEvaluator(labelCol = "mitochondrial_myopathy",
                                                 metricName = "weightedPrecision").evaluate(df_predictions1)

df_precision1

"""The weighted precision of this model is about 60%. """

df_classifier1.featureImportances

"""There are no remarkable features in this model. This could be changed with more instances of the disease in the dataset."""

df_auc1 = MulticlassClassificationEvaluator(labelCol = "mitochondrial_myopathy").evaluate(df_predictions1)
df_auc1

"""The AUC of this model is about 68%. 

This model could use improvements whether it is a larger amount of instances or focusing on different features.

**Cystic Fibrosis Decision Tree**
"""

model_df2 = output.select("features", "cystic_fibrosis")

training_df2, test_df2 = model_df2.randomSplit([0.7, 0.3], 1000)

df_classifier2 = DecisionTreeClassifier(labelCol = "cystic_fibrosis").fit(training_df2)

df_predictions2 = df_classifier2.transform(test_df2)

df_accuracy2 = MulticlassClassificationEvaluator(labelCol="cystic_fibrosis",
                                                metricName= "accuracy").evaluate(df_predictions2)

df_accuracy2

"""The accuracy of this model is about 82% for being able to predict patients with Cystic Fibrosis."""

df_precision2 = MulticlassClassificationEvaluator(labelCol = "cystic_fibrosis",
                                                 metricName = "weightedPrecision").evaluate(df_predictions2)

df_precision2

"""The weighted precision of this model is about 67%. """

df_classifier2.featureImportances

"""There are no remarkable features in this model. This could be changed with more instances of the disease in the dataset. """

df_auc2 = MulticlassClassificationEvaluator(labelCol = "cystic_fibrosis").evaluate(df_predictions2)
df_auc2

"""The AUC of this model is about 74%. 

This model is acceptable, but could use some improvements. This could come from a larger sample size or possibility of more features.

**Tay-Sachs Decision Tree**
"""

model_df3 = output.select("features", "tay_sachs")

training_df3, test_df3 = model_df3.randomSplit([0.7, 0.3], 1000)

df_classifier3 = DecisionTreeClassifier(labelCol = "tay_sachs").fit(training_df3)

df_predictions3 = df_classifier3.transform(test_df3)

df_accuracy3 = MulticlassClassificationEvaluator(labelCol="tay_sachs",
                                                metricName= "accuracy").evaluate(df_predictions3)

df_accuracy3

"""The accuracy of this model is 86% for predicting Tay-Sachs."""

df_precision3 = MulticlassClassificationEvaluator(labelCol = "tay_sachs",
                                                 metricName = "weightedPrecision").evaluate(df_predictions3)

df_precision3

"""The weighted precision of this model is 74%."""

df_classifier3.featureImportances

"""No remarkable features in this model. """

df_auc3 = MulticlassClassificationEvaluator(labelCol = "tay_sachs").evaluate(df_predictions3)
df_auc3

"""The AUC of this model is 80%. This model is very good for predicting patients with Tay-Sachs.

**Diabetes Decision Tree**
"""

model_df4 = output.select("features", "diabetes")

training_df4, test_df4 = model_df4.randomSplit([0.7, 0.3], 1000)

df_classifier4 = DecisionTreeClassifier(labelCol = "diabetes").fit(training_df4)

df_predictions4 = df_classifier4.transform(test_df4)

df_accuracy4 = MulticlassClassificationEvaluator(labelCol="diabetes",
                                                metricName= "accuracy").evaluate(df_predictions4)

df_accuracy4

"""The accuracy of this model is 90%, which is very high. """

df_precision4 = MulticlassClassificationEvaluator(labelCol = "diabetes",
                                                 metricName = "weightedPrecision").evaluate(df_predictions4)

df_precision4

"""The weighted precision is about 83%. """

df_classifier4.featureImportances

"""No remarkable features in this model."""

df_auc4 = MulticlassClassificationEvaluator(labelCol = "diabetes").evaluate(df_predictions4)
df_auc4

"""The AUC of this model is 87%, which is very good.

**Hemochromatosis Decision Tree**
"""

model_df5 = output.select("features", "hemochromatosis")

training_df5, test_df5 = model_df5.randomSplit([0.7, 0.3], 1000)

df_classifier5 = DecisionTreeClassifier(labelCol = "hemochromatosis").fit(training_df5)

df_predictions5 = df_classifier5.transform(test_df5)

df_accuracy5 = MulticlassClassificationEvaluator(labelCol="hemochromatosis",
                                                metricName= "accuracy").evaluate(df_predictions5)

df_accuracy5

"""The accuracy of this model is 93% when predicting Hemochromatosis"""

df_precision5 = MulticlassClassificationEvaluator(labelCol = "hemochromatosis",
                                                 metricName = "weightedPrecision").evaluate(df_predictions5)

df_precision5

"""The weighted precision of this model is 87%. """

df_classifier5.featureImportances

"""No remarkable features. """

df_auc5 = MulticlassClassificationEvaluator(labelCol = "hemochromatosis").evaluate(df_predictions5)
df_auc5

"""The AUC of this model is 90%, which is very good.

**Leber's Hereditary Optic Neuropathy Decision Tree**
"""

model_df6 = output.select("features", "lebers")

training_df6, test_df6 = model_df6.randomSplit([0.7, 0.3], 1000)

df_classifier6 = DecisionTreeClassifier(labelCol = "lebers").fit(training_df6)

df_predictions6 = df_classifier6.transform(test_df6)

df_accuracy6 = MulticlassClassificationEvaluator(labelCol="lebers",
                                                metricName= "accuracy").evaluate(df_predictions6)

df_accuracy6

"""The accuracy of this model is 97% when prediciting Leber's. """

df_precision6 = MulticlassClassificationEvaluator(labelCol = "lebers",
                                                 metricName = "weightedPrecision").evaluate(df_predictions6)

df_precision6

"""The weighted precision of this model is about 94%."""

df_classifier6.featureImportances

"""No remarkable features. """

df_auc6 = MulticlassClassificationEvaluator(labelCol = "lebers").evaluate(df_predictions6)
df_auc6

"""The AUC of this model is 95%. As the number of instances decrease, we notice the evaluation measures for the models increase.

**Alzheimer's Decision Tree**
"""

model_df7 = output.select("features", "alzheimers")

training_df7, test_df7 = model_df7.randomSplit([0.7, 0.3], 1000)

df_classifier7 = DecisionTreeClassifier(labelCol = "alzheimers").fit(training_df7)

df_predictions7 = df_classifier7.transform(test_df7)

df_accuracy7 = MulticlassClassificationEvaluator(labelCol="alzheimers",
                                                metricName= "accuracy").evaluate(df_predictions7)

df_accuracy7

"""The accuracy of this model is 99%. This is due to the small amount of instances in the dataset."""

df_precision7 = MulticlassClassificationEvaluator(labelCol = "alzheimers",
                                                 metricName = "weightedPrecision").evaluate(df_predictions7)

df_precision7

"""The weighted precision of the model is 98%. """

df_classifier7.featureImportances

"""No remarkable features. """

df_auc7 = MulticlassClassificationEvaluator(labelCol = "alzheimers").evaluate(df_predictions7)
df_auc7

"""The AUC of the model is 98%. As previously mentioned, the evaluaters of the models have increased as the number of instances has decreased. We would most likely get more accurate models with more instances of these diseases.

**Cancer Decision Tree**
"""

model_df8 = output.select("features", "cancer")

training_df8, test_df8 = model_df8.randomSplit([0.7, 0.3], 1000)

df_classifier8 = DecisionTreeClassifier(labelCol = "cancer").fit(training_df8)

df_predictions8 = df_classifier8.transform(test_df8)

df_accuracy8 = MulticlassClassificationEvaluator(labelCol="cancer",
                                                metricName= "accuracy").evaluate(df_predictions8)

df_accuracy8

"""The accuracy of the model is 99% for predicting Cancer."""

df_precision8 = MulticlassClassificationEvaluator(labelCol = "cancer",
                                                 metricName = "weightedPrecision").evaluate(df_predictions8)

df_precision8

"""The weighted precision of the model is also 99%. """

df_classifier8.featureImportances

"""No remarkable features. """

df_auc8 = MulticlassClassificationEvaluator(labelCol = "cancer").evaluate(df_predictions8)
df_auc8

"""The AUC of the model is 99%. 

We are more likely to get a better model with less overfitting with more instances of the diseases.

Overall, the models could be improved with more instances of the diseases with could come from data collection or simulated instances to create better predictions.

**Resources:**

https://medium.com/radon-dev/item-item-collaborative-filtering-with-binary-or-unary-data-e8f0b465b2c3

https://academic.oup.com/bioinformatics/article/34/22/3907/5026663

https://towardsdatascience.com/recommender-systems-item-customer-collaborative-filtering-ff0c8f41ae8a


https://medium.com/analytics-vidhya/linear-regression-and-decision-tree-implementation-using-pyspark-bfcd93dee86

https://python.plainenglish.io/decision-trees-random-forests-in-pyspark-d07546e4fa7d

https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html

https://www.datatechnotes.com/2021/06/pyspark-decision-tree-classification.html

https://towardsdatascience.com/choosing-performance-metrics-61b40819eae1
"""
